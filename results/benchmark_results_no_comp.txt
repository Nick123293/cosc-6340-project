==================================================
CONFIG 1: feb_march_seq9_future3_parsed
  range_tag   = feb_march
  mode        = parsed
  train_csv   = data/training_data_parsed_feb_march.csv
  base_args   = --seq-len-in 9 --future-steps 3
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-03-01 00:00:00
--------------------------------------------------
[CONFIG 1] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.117687  val=0.001409
Model saved at: checkpoints/checkpoint.pth
Total runtime: 18.81 seconds
Experiment metrics saved to results/feb_march_seq9_future3_parsed_run1.json
  Run 1 metrics:
    runtime_s = 18.811830520629883
    train     = 0.11768679531242253
    val       = 0.0014086313312873244

[CONFIG 1] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.117278  val=0.001695
Model saved at: checkpoints/checkpoint.pth
Total runtime: 18.95 seconds
Experiment metrics saved to results/feb_march_seq9_future3_parsed_run2.json
  Run 2 metrics:
    runtime_s = 18.945494890213013
    train     = 0.11727757863265954
    val       = 0.0016953330487012863

>>> AVERAGES for CONFIG 1 over 2 runs:
    config_label = feb_march_seq9_future3_parsed
    base_args    = --seq-len-in 9 --future-steps 3
    avg_time_s   = 18.88
    avg_train    = 0.117482
    avg_val      = 0.001552

==================================================
CONFIG 2: feb_march_seq9_future3_full
  range_tag   = feb_march
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-03-01 00:00:00
--------------------------------------------------
[CONFIG 2] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-03-01 00:00:00 (697 hours).
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.120936  val=0.001578
Model saved at: checkpoints/checkpoint.pth
Total runtime: 57.70 seconds
Experiment metrics saved to results/feb_march_seq9_future3_full_run1.json
  Run 1 metrics:
    runtime_s = 57.69547367095947
    train     = 0.1209358368014629
    val       = 0.0015781078254804015

[CONFIG 2] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-03-01 00:00:00 (697 hours).
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.114253  val=0.001872
Model saved at: checkpoints/checkpoint.pth
Total runtime: 54.41 seconds
Experiment metrics saved to results/feb_march_seq9_future3_full_run2.json
  Run 2 metrics:
    runtime_s = 54.40556049346924
    train     = 0.11425285321815108
    val       = 0.0018715092446655035

>>> AVERAGES for CONFIG 2 over 2 runs:
    config_label = feb_march_seq9_future3_full
    base_args    = --seq-len-in 9 --future-steps 3
    avg_time_s   = 56.05
    avg_train    = 0.117594
    avg_val      = 0.001725

==================================================
CONFIG 3: feb_april_seq9_future3_parsed
  range_tag   = feb_april
  mode        = parsed
  train_csv   = data/training_data_parsed_feb_april.csv
  base_args   = --seq-len-in 9 --future-steps 3
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-04-01 00:00:00
--------------------------------------------------
[CONFIG 3] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.100719  val=0.010247
Model saved at: checkpoints/checkpoint.pth
Total runtime: 36.84 seconds
Experiment metrics saved to results/feb_april_seq9_future3_parsed_run1.json
  Run 1 metrics:
    runtime_s = 36.84438419342041
    train     = 0.10071851439396297
    val       = 0.010246959514915943

[CONFIG 3] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.101039  val=0.010530
Model saved at: checkpoints/checkpoint.pth
Total runtime: 37.28 seconds
Experiment metrics saved to results/feb_april_seq9_future3_parsed_run2.json
  Run 2 metrics:
    runtime_s = 37.27737903594971
    train     = 0.10103863858572253
    val       = 0.010529502294957638

>>> AVERAGES for CONFIG 3 over 2 runs:
    config_label = feb_april_seq9_future3_parsed
    base_args    = --seq-len-in 9 --future-steps 3
    avg_time_s   = 37.06
    avg_train    = 0.100879
    avg_val      = 0.010388

==================================================
CONFIG 4: feb_april_seq9_future3_full
  range_tag   = feb_april
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-04-01 00:00:00
--------------------------------------------------
[CONFIG 4] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-04-01 00:00:00 (1441 hours).
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.101000  val=0.009802
Model saved at: checkpoints/checkpoint.pth
Total runtime: 67.08 seconds
Experiment metrics saved to results/feb_april_seq9_future3_full_run1.json
  Run 1 metrics:
    runtime_s = 67.07969784736633
    train     = 0.10099976235232153
    val       = 0.009802059270441532

[CONFIG 4] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-04-01 00:00:00 (1441 hours).
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.100691  val=0.010342
Model saved at: checkpoints/checkpoint.pth
Total runtime: 72.55 seconds
Experiment metrics saved to results/feb_april_seq9_future3_full_run2.json
  Run 2 metrics:
    runtime_s = 72.54725241661072
    train     = 0.1006914018223492
    val       = 0.010342116467654705

>>> AVERAGES for CONFIG 4 over 2 runs:
    config_label = feb_april_seq9_future3_full
    base_args    = --seq-len-in 9 --future-steps 3
    avg_time_s   = 69.81
    avg_train    = 0.100846
    avg_val      = 0.010072

==================================================
CONFIG 5: july_august_seq9_future3_parsed
  range_tag   = july_august
  mode        = parsed
  train_csv   = data/training_data_parsed_july_august.csv
  base_args   = --seq-len-in 9 --future-steps 3
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-08-01 00:00:00
--------------------------------------------------
[CONFIG 5] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.28 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.094517  val=0.005271
Model saved at: checkpoints/checkpoint.pth
Total runtime: 20.91 seconds
Experiment metrics saved to results/july_august_seq9_future3_parsed_run1.json
  Run 1 metrics:
    runtime_s = 20.908939599990845
    train     = 0.09451690472393456
    val       = 0.005271077621728182

[CONFIG 5] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.28 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.097388  val=0.004660
Model saved at: checkpoints/checkpoint.pth
Total runtime: 20.45 seconds
Experiment metrics saved to results/july_august_seq9_future3_parsed_run2.json
  Run 2 metrics:
    runtime_s = 20.44883632659912
    train     = 0.09738776109204184
    val       = 0.004659811966121197

>>> AVERAGES for CONFIG 5 over 2 runs:
    config_label = july_august_seq9_future3_parsed
    base_args    = --seq-len-in 9 --future-steps 3
    avg_time_s   = 20.68
    avg_train    = 0.095952
    avg_val      = 0.004965

==================================================
CONFIG 6: july_august_seq9_future3_full
  range_tag   = july_august
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-08-01 00:00:00
--------------------------------------------------
[CONFIG 6] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-08-01 00:00:00 (745 hours).
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.104193  val=0.004503
Model saved at: checkpoints/checkpoint.pth
Total runtime: 59.64 seconds
Experiment metrics saved to results/july_august_seq9_future3_full_run1.json
  Run 1 metrics:
    runtime_s = 59.643593549728394
    train     = 0.10419274268326748
    val       = 0.004503409843891859

[CONFIG 6] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-08-01 00:00:00 (745 hours).
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.097711  val=0.001994
Model saved at: checkpoints/checkpoint.pth
Total runtime: 54.87 seconds
Experiment metrics saved to results/july_august_seq9_future3_full_run2.json
  Run 2 metrics:
    runtime_s = 54.86605525016785
    train     = 0.09771134952534934
    val       = 0.0019941930659115314

>>> AVERAGES for CONFIG 6 over 2 runs:
    config_label = july_august_seq9_future3_full
    base_args    = --seq-len-in 9 --future-steps 3
    avg_time_s   = 57.25
    avg_train    = 0.100952
    avg_val      = 0.003249

==================================================
CONFIG 7: july_september_seq9_future3_parsed
  range_tag   = july_september
  mode        = parsed
  train_csv   = data/training_data_parsed_july_september.csv
  base_args   = --seq-len-in 9 --future-steps 3
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-09-01 00:00:00
--------------------------------------------------
[CONFIG 7] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.42 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 720
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.119711  val=0.010839
Model saved at: checkpoints/checkpoint.pth
Total runtime: 19.47 seconds
Experiment metrics saved to results/july_september_seq9_future3_parsed_run1.json
  Run 1 metrics:
    runtime_s = 19.473900079727173
    train     = 0.11971068589282137
    val       = 0.010839487425982952

[CONFIG 7] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.42 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 720
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.119188  val=0.028727
Model saved at: checkpoints/checkpoint.pth
Total runtime: 19.67 seconds
Experiment metrics saved to results/july_september_seq9_future3_parsed_run2.json
  Run 2 metrics:
    runtime_s = 19.665512561798096
    train     = 0.11918824862252674
    val       = 0.028727484866976738

>>> AVERAGES for CONFIG 7 over 2 runs:
    config_label = july_september_seq9_future3_parsed
    base_args    = --seq-len-in 9 --future-steps 3
    avg_time_s   = 19.57
    avg_train    = 0.119449
    avg_val      = 0.019783

==================================================
CONFIG 8: july_september_seq9_future3_full
  range_tag   = july_september
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-09-01 00:00:00
--------------------------------------------------
[CONFIG 8] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-09-01 00:00:00 (1489 hours).
[MEM] Effective dense hourly timesteps (after filter): 1489
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.069845  val=0.002724
Model saved at: checkpoints/checkpoint.pth
Total runtime: 73.36 seconds
Experiment metrics saved to results/july_september_seq9_future3_full_run1.json
  Run 1 metrics:
    runtime_s = 73.35626101493835
    train     = 0.06984495404661609
    val       = 0.0027243115473538637

[CONFIG 8] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-09-01 00:00:00 (1489 hours).
[MEM] Effective dense hourly timesteps (after filter): 1489
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.073593  val=0.005003
Model saved at: checkpoints/checkpoint.pth
Total runtime: 68.98 seconds
Experiment metrics saved to results/july_september_seq9_future3_full_run2.json
  Run 2 metrics:
    runtime_s = 68.98251748085022
    train     = 0.07359348110780901
    val       = 0.005003394093364477

>>> AVERAGES for CONFIG 8 over 2 runs:
    config_label = july_september_seq9_future3_full
    base_args    = --seq-len-in 9 --future-steps 3
    avg_time_s   = 71.17
    avg_train    = 0.071719
    avg_val      = 0.003864

==================================================
CONFIG 9: feb_march_seq9_future3_ram128M_parsed
  range_tag   = feb_march
  mode        = parsed
  train_csv   = data/training_data_parsed_feb_march.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-03-01 00:00:00
--------------------------------------------------
[CONFIG 9] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.120377  val=0.001833
Model saved at: checkpoints/checkpoint.pth
Total runtime: 23.55 seconds
Experiment metrics saved to results/feb_march_seq9_future3_ram128M_parsed_run1.json
  Run 1 metrics:
    runtime_s = 23.551082611083984
    train     = 0.12037712647837556
    val       = 0.0018327938159927726

[CONFIG 9] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.115339  val=0.001700
Model saved at: checkpoints/checkpoint.pth
Total runtime: 22.53 seconds
Experiment metrics saved to results/feb_march_seq9_future3_ram128M_parsed_run2.json
  Run 2 metrics:
    runtime_s = 22.528535842895508
    train     = 0.11533852898768276
    val       = 0.001699779531918466

>>> AVERAGES for CONFIG 9 over 2 runs:
    config_label = feb_march_seq9_future3_ram128M_parsed
    base_args    = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
    avg_time_s   = 23.04
    avg_train    = 0.117858
    avg_val      = 0.001766

==================================================
CONFIG 10: feb_march_seq9_future3_ram128M_full
  range_tag   = feb_march
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-03-01 00:00:00
--------------------------------------------------
[CONFIG 10] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-03-01 00:00:00 (697 hours).
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.116558  val=0.002021
Model saved at: checkpoints/checkpoint.pth
Total runtime: 74.37 seconds
Experiment metrics saved to results/feb_march_seq9_future3_ram128M_full_run1.json
  Run 1 metrics:
    runtime_s = 74.3727912902832
    train     = 0.11655804913751672
    val       = 0.0020205131731927395

[CONFIG 10] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-03-01 00:00:00 (697 hours).
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.118910  val=0.002103
Model saved at: checkpoints/checkpoint.pth
Total runtime: 81.59 seconds
Experiment metrics saved to results/feb_march_seq9_future3_ram128M_full_run2.json
  Run 2 metrics:
    runtime_s = 81.59398436546326
    train     = 0.11890963920590307
    val       = 0.002103236736729741

>>> AVERAGES for CONFIG 10 over 2 runs:
    config_label = feb_march_seq9_future3_ram128M_full
    base_args    = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
    avg_time_s   = 77.98
    avg_train    = 0.117734
    avg_val      = 0.002062

==================================================
CONFIG 11: feb_april_seq9_future3_ram128M_parsed
  range_tag   = feb_april
  mode        = parsed
  train_csv   = data/training_data_parsed_feb_april.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-04-01 00:00:00
--------------------------------------------------
[CONFIG 11] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.101747  val=0.010852
Model saved at: checkpoints/checkpoint.pth
Total runtime: 54.11 seconds
Experiment metrics saved to results/feb_april_seq9_future3_ram128M_parsed_run1.json
  Run 1 metrics:
    runtime_s = 54.108073711395264
    train     = 0.10174705558944937
    val       = 0.01085247378796339

[CONFIG 11] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.099948  val=0.009441
Model saved at: checkpoints/checkpoint.pth
Total runtime: 55.35 seconds
Experiment metrics saved to results/feb_april_seq9_future3_ram128M_parsed_run2.json
  Run 2 metrics:
    runtime_s = 55.35078716278076
    train     = 0.09994786855523716
    val       = 0.009441311471164227

>>> AVERAGES for CONFIG 11 over 2 runs:
    config_label = feb_april_seq9_future3_ram128M_parsed
    base_args    = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
    avg_time_s   = 54.73
    avg_train    = 0.100847
    avg_val      = 0.010147

==================================================
CONFIG 12: feb_april_seq9_future3_ram128M_full
  range_tag   = feb_april
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-04-01 00:00:00
--------------------------------------------------
[CONFIG 12] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-04-01 00:00:00 (1441 hours).
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.101304  val=0.010429
Model saved at: checkpoints/checkpoint.pth
Total runtime: 132.22 seconds
Experiment metrics saved to results/feb_april_seq9_future3_ram128M_full_run1.json
  Run 1 metrics:
    runtime_s = 132.21594500541687
    train     = 0.10130357293776802
    val       = 0.010428721085190773

[CONFIG 12] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-04-01 00:00:00 (1441 hours).
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.101394  val=0.009662
Model saved at: checkpoints/checkpoint.pth
Total runtime: 132.98 seconds
Experiment metrics saved to results/feb_april_seq9_future3_ram128M_full_run2.json
  Run 2 metrics:
    runtime_s = 132.97639966011047
    train     = 0.10139396858264829
    val       = 0.009661582298576832

>>> AVERAGES for CONFIG 12 over 2 runs:
    config_label = feb_april_seq9_future3_ram128M_full
    base_args    = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
    avg_time_s   = 132.60
    avg_train    = 0.101349
    avg_val      = 0.010045

==================================================
CONFIG 13: july_august_seq9_future3_ram128M_parsed
  range_tag   = july_august
  mode        = parsed
  train_csv   = data/training_data_parsed_july_august.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-08-01 00:00:00
--------------------------------------------------
[CONFIG 13] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.28 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.102472  val=0.003356
Model saved at: checkpoints/checkpoint.pth
Total runtime: 29.28 seconds
Experiment metrics saved to results/july_august_seq9_future3_ram128M_parsed_run1.json
  Run 1 metrics:
    runtime_s = 29.27668809890747
    train     = 0.10247169401019568
    val       = 0.0033558194991201162

[CONFIG 13] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.28 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.094479  val=0.009055
Model saved at: checkpoints/checkpoint.pth
Total runtime: 28.27 seconds
Experiment metrics saved to results/july_august_seq9_future3_ram128M_parsed_run2.json
  Run 2 metrics:
    runtime_s = 28.269638061523438
    train     = 0.09447929432630864
    val       = 0.009054825641214848

>>> AVERAGES for CONFIG 13 over 2 runs:
    config_label = july_august_seq9_future3_ram128M_parsed
    base_args    = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
    avg_time_s   = 28.77
    avg_train    = 0.098475
    avg_val      = 0.006205

==================================================
CONFIG 14: july_august_seq9_future3_ram128M_full
  range_tag   = july_august
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-08-01 00:00:00
--------------------------------------------------
[CONFIG 14] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-08-01 00:00:00 (745 hours).
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.097513  val=0.006105
Model saved at: checkpoints/checkpoint.pth
Total runtime: 82.58 seconds
Experiment metrics saved to results/july_august_seq9_future3_ram128M_full_run1.json
  Run 1 metrics:
    runtime_s = 82.58461046218872
    train     = 0.09751278882825935
    val       = 0.00610453961417079

[CONFIG 14] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-08-01 00:00:00 (745 hours).
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.096908  val=0.010237
Model saved at: checkpoints/checkpoint.pth
Total runtime: 82.13 seconds
Experiment metrics saved to results/july_august_seq9_future3_ram128M_full_run2.json
  Run 2 metrics:
    runtime_s = 82.12659120559692
    train     = 0.09690846369299805
    val       = 0.010237242095172405

>>> AVERAGES for CONFIG 14 over 2 runs:
    config_label = july_august_seq9_future3_ram128M_full
    base_args    = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
    avg_time_s   = 82.36
    avg_train    = 0.097211
    avg_val      = 0.008171

==================================================
CONFIG 15: july_september_seq9_future3_ram128M_parsed
  range_tag   = july_september
  mode        = parsed
  train_csv   = data/training_data_parsed_july_september.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-09-01 00:00:00
--------------------------------------------------
[CONFIG 15] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.42 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 720
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.115289  val=0.010522
Model saved at: checkpoints/checkpoint.pth
Total runtime: 24.30 seconds
Experiment metrics saved to results/july_september_seq9_future3_ram128M_parsed_run1.json
  Run 1 metrics:
    runtime_s = 24.295711278915405
    train     = 0.1152891193389136
    val       = 0.01052212156355381

[CONFIG 15] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.42 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 720
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.111392  val=0.016047
Model saved at: checkpoints/checkpoint.pth
Total runtime: 23.39 seconds
Experiment metrics saved to results/july_september_seq9_future3_ram128M_parsed_run2.json
  Run 2 metrics:
    runtime_s = 23.387629747390747
    train     = 0.11139154630189049
    val       = 0.016046542674303055

>>> AVERAGES for CONFIG 15 over 2 runs:
    config_label = july_september_seq9_future3_ram128M_parsed
    base_args    = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
    avg_time_s   = 23.84
    avg_train    = 0.113340
    avg_val      = 0.013284

==================================================
CONFIG 16: july_september_seq9_future3_ram128M_full
  range_tag   = july_september
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-09-01 00:00:00
--------------------------------------------------
[CONFIG 16] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-09-01 00:00:00 (1489 hours).
[MEM] Effective dense hourly timesteps (after filter): 1489
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.069547  val=0.001833
Model saved at: checkpoints/checkpoint.pth
Total runtime: 163.15 seconds
Experiment metrics saved to results/july_september_seq9_future3_ram128M_full_run1.json
  Run 1 metrics:
    runtime_s = 163.15308451652527
    train     = 0.06954715202886394
    val       = 0.0018334176857024431

[CONFIG 16] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-09-01 00:00:00 (1489 hours).
[MEM] Effective dense hourly timesteps (after filter): 1489
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.069032  val=0.001995
Model saved at: checkpoints/checkpoint.pth
Total runtime: 164.40 seconds
Experiment metrics saved to results/july_september_seq9_future3_ram128M_full_run2.json
  Run 2 metrics:
    runtime_s = 164.39860486984253
    train     = 0.0690323055970088
    val       = 0.0019946633838117123

>>> AVERAGES for CONFIG 16 over 2 runs:
    config_label = july_september_seq9_future3_ram128M_full
    base_args    = --seq-len-in 9 --future-steps 3 --max-ram-bytes 134217728
    avg_time_s   = 163.78
    avg_train    = 0.069290
    avg_val      = 0.001914

==================================================
CONFIG 17: feb_march_seq9_future3_vram128M_parsed
  range_tag   = feb_march
  mode        = parsed
  train_csv   = data/training_data_parsed_feb_march.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-03-01 00:00:00
--------------------------------------------------
[CONFIG 17] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.116280  val=0.001482
Model saved at: checkpoints/checkpoint.pth
Total runtime: 23.40 seconds
Experiment metrics saved to results/feb_march_seq9_future3_vram128M_parsed_run1.json
  Run 1 metrics:
    runtime_s = 23.39968252182007
    train     = 0.11627994323252502
    val       = 0.0014816386392340064

[CONFIG 17] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.119181  val=0.002207
Model saved at: checkpoints/checkpoint.pth
Total runtime: 23.29 seconds
Experiment metrics saved to results/feb_march_seq9_future3_vram128M_parsed_run2.json
  Run 2 metrics:
    runtime_s = 23.28873920440674
    train     = 0.11918105918414477
    val       = 0.0022065944503992796

>>> AVERAGES for CONFIG 17 over 2 runs:
    config_label = feb_march_seq9_future3_vram128M_parsed
    base_args    = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
    avg_time_s   = 23.34
    avg_train    = 0.117731
    avg_val      = 0.001844

==================================================
CONFIG 18: feb_march_seq9_future3_vram128M_full
  range_tag   = feb_march
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-03-01 00:00:00
--------------------------------------------------
[CONFIG 18] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-03-01 00:00:00 (697 hours).
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.114763  val=0.001909
Model saved at: checkpoints/checkpoint.pth
Total runtime: 81.93 seconds
Experiment metrics saved to results/feb_march_seq9_future3_vram128M_full_run1.json
  Run 1 metrics:
    runtime_s = 81.92745232582092
    train     = 0.11476340584739304
    val       = 0.0019090069690719247

[CONFIG 18] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-03-01 00:00:00 (697 hours).
[MEM] Effective dense hourly timesteps (after filter): 697
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.115044  val=0.001423
Model saved at: checkpoints/checkpoint.pth
Total runtime: 82.61 seconds
Experiment metrics saved to results/feb_march_seq9_future3_vram128M_full_run2.json
  Run 2 metrics:
    runtime_s = 82.60864400863647
    train     = 0.11504447068095902
    val       = 0.0014232760295271873

>>> AVERAGES for CONFIG 18 over 2 runs:
    config_label = feb_march_seq9_future3_vram128M_full
    base_args    = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
    avg_time_s   = 82.27
    avg_train    = 0.114904
    avg_val      = 0.001666

==================================================
CONFIG 19: feb_april_seq9_future3_vram128M_parsed
  range_tag   = feb_april
  mode        = parsed
  train_csv   = data/training_data_parsed_feb_april.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-04-01 00:00:00
--------------------------------------------------
[CONFIG 19] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.100922  val=0.012370
Model saved at: checkpoints/checkpoint.pth
Total runtime: 49.05 seconds
Experiment metrics saved to results/feb_april_seq9_future3_vram128M_parsed_run1.json
  Run 1 metrics:
    runtime_s = 49.054003953933716
    train     = 0.10092215322648417
    val       = 0.0123696094378829

[CONFIG 19] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.70 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.102902  val=0.010256
Model saved at: checkpoints/checkpoint.pth
Total runtime: 48.61 seconds
Experiment metrics saved to results/feb_april_seq9_future3_vram128M_parsed_run2.json
  Run 2 metrics:
    runtime_s = 48.60713791847229
    train     = 0.10290216893231223
    val       = 0.01025573443621397

>>> AVERAGES for CONFIG 19 over 2 runs:
    config_label = feb_april_seq9_future3_vram128M_parsed
    base_args    = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
    avg_time_s   = 48.83
    avg_train    = 0.101912
    avg_val      = 0.011313

==================================================
CONFIG 20: feb_april_seq9_future3_vram128M_full
  range_tag   = feb_april
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
  time_start  = 2024-02-01 00:00:00
  time_end    = 2024-04-01 00:00:00
--------------------------------------------------
[CONFIG 20] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-04-01 00:00:00 (1441 hours).
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.101560  val=0.010747
Model saved at: checkpoints/checkpoint.pth
Total runtime: 114.58 seconds
Experiment metrics saved to results/feb_april_seq9_future3_vram128M_full_run1.json
  Run 1 metrics:
    runtime_s = 114.57509660720825
    train     = 0.10155987796158744
    val       = 0.01074696984142065

[CONFIG 20] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-02-01 00:00:00 .. 2024-04-01 00:00:00 (1441 hours).
[MEM] Effective dense hourly timesteps (after filter): 1441
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.101101  val=0.010659
Model saved at: checkpoints/checkpoint.pth
Total runtime: 120.44 seconds
Experiment metrics saved to results/feb_april_seq9_future3_vram128M_full_run2.json
  Run 2 metrics:
    runtime_s = 120.43912744522095
    train     = 0.10110096904559882
    val       = 0.010658752173185349

>>> AVERAGES for CONFIG 20 over 2 runs:
    config_label = feb_april_seq9_future3_vram128M_full
    base_args    = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
    avg_time_s   = 117.51
    avg_train    = 0.101330
    avg_val      = 0.010703

==================================================
CONFIG 21: july_august_seq9_future3_vram128M_parsed
  range_tag   = july_august
  mode        = parsed
  train_csv   = data/training_data_parsed_july_august.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-08-01 00:00:00
--------------------------------------------------
[CONFIG 21] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.28 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.094213  val=0.007995
Model saved at: checkpoints/checkpoint.pth
Total runtime: 25.24 seconds
Experiment metrics saved to results/july_august_seq9_future3_vram128M_parsed_run1.json
  Run 1 metrics:
    runtime_s = 25.24000120162964
    train     = 0.09421290040564309
    val       = 0.007994669489562511

[CONFIG 21] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.28 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.097938  val=0.010407
Model saved at: checkpoints/checkpoint.pth
Total runtime: 24.99 seconds
Experiment metrics saved to results/july_august_seq9_future3_vram128M_parsed_run2.json
  Run 2 metrics:
    runtime_s = 24.991000175476074
    train     = 0.09793821492737377
    val       = 0.01040734350681305

>>> AVERAGES for CONFIG 21 over 2 runs:
    config_label = july_august_seq9_future3_vram128M_parsed
    base_args    = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
    avg_time_s   = 25.12
    avg_train    = 0.096076
    avg_val      = 0.009201

==================================================
CONFIG 22: july_august_seq9_future3_vram128M_full
  range_tag   = july_august
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-08-01 00:00:00
--------------------------------------------------
[CONFIG 22] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-08-01 00:00:00 (745 hours).
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.105957  val=0.006623
Model saved at: checkpoints/checkpoint.pth
Total runtime: 82.71 seconds
Experiment metrics saved to results/july_august_seq9_future3_vram128M_full_run1.json
  Run 1 metrics:
    runtime_s = 82.70521712303162
    train     = 0.10595694175928994
    val       = 0.006622727960348129

[CONFIG 22] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-08-01 00:00:00 (745 hours).
[MEM] Effective dense hourly timesteps (after filter): 745
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.097075  val=0.004383
Model saved at: checkpoints/checkpoint.pth
Total runtime: 81.20 seconds
Experiment metrics saved to results/july_august_seq9_future3_vram128M_full_run2.json
  Run 2 metrics:
    runtime_s = 81.20329260826111
    train     = 0.09707506148814546
    val       = 0.004383288323879242

>>> AVERAGES for CONFIG 22 over 2 runs:
    config_label = july_august_seq9_future3_vram128M_full
    base_args    = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
    avg_time_s   = 81.95
    avg_train    = 0.101516
    avg_val      = 0.005503

==================================================
CONFIG 23: july_september_seq9_future3_vram128M_parsed
  range_tag   = july_september
  mode        = parsed
  train_csv   = data/training_data_parsed_july_september.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-09-01 00:00:00
--------------------------------------------------
[CONFIG 23] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.42 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 720
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.114951  val=0.021112
Model saved at: checkpoints/checkpoint.pth
Total runtime: 23.23 seconds
Experiment metrics saved to results/july_september_seq9_future3_vram128M_parsed_run1.json
  Run 1 metrics:
    runtime_s = 23.232565879821777
    train     = 0.11495118422173646
    val       = 0.02111208625137806

[CONFIG 23] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=70.42 bytes, min=52 bytes, max=113 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[MEM] Effective dense hourly timesteps (after filter): 720
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.115304  val=0.012010
Model saved at: checkpoints/checkpoint.pth
Total runtime: 24.14 seconds
Experiment metrics saved to results/july_september_seq9_future3_vram128M_parsed_run2.json
  Run 2 metrics:
    runtime_s = 24.137351036071777
    train     = 0.1153041360734296
    val       = 0.012010358273983002

>>> AVERAGES for CONFIG 23 over 2 runs:
    config_label = july_september_seq9_future3_vram128M_parsed
    base_args    = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
    avg_time_s   = 23.68
    avg_train    = 0.115128
    avg_val      = 0.016561

==================================================
CONFIG 24: july_september_seq9_future3_vram128M_full
  range_tag   = july_september
  mode        = full
  train_csv   = data/training_data.csv
  base_args   = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
  time_start  = 2024-07-01 00:00:00
  time_end    = 2024-09-01 00:00:00
--------------------------------------------------
[CONFIG 24] Run 1...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-09-01 00:00:00 (1489 hours).
[MEM] Effective dense hourly timesteps (after filter): 1489
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.070814  val=0.002253
Model saved at: checkpoints/checkpoint.pth
Total runtime: 118.02 seconds
Experiment metrics saved to results/july_september_seq9_future3_vram128M_full_run1.json
  Run 1 metrics:
    runtime_s = 118.02420043945312
    train     = 0.07081385956244123
    val       = 0.0022529209963977337

[CONFIG 24] Run 2...
[MEM] Row size stats over 1000 sampled rows (excluding header): avg=69.53 bytes, min=52 bytes, max=112 bytes
[MEM] Scanning CSV for global structure with chunksize=100000 rows...
[FILTER] Restricting training to datetimes 2024-07-01 00:00:00 .. 2024-09-01 00:00:00 (1489 hours).
[MEM] Effective dense hourly timesteps (after filter): 1489
[MEM] Approx dense bytes per timestep: 260176.00
[EPOCH 1/1] train=0.069515  val=0.001988
Model saved at: checkpoints/checkpoint.pth
Total runtime: 110.52 seconds
Experiment metrics saved to results/july_september_seq9_future3_vram128M_full_run2.json
  Run 2 metrics:
    runtime_s = 110.51672220230103
    train     = 0.06951492794405628
    val       = 0.001987698022276163

>>> AVERAGES for CONFIG 24 over 2 runs:
    config_label = july_september_seq9_future3_vram128M_full
    base_args    = --seq-len-in 9 --future-steps 3 --max-vram-bytes 134217728
    avg_time_s   = 114.27
    avg_train    = 0.070164
    avg_val      = 0.002120

