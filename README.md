Instructions to use our code:  
1) To create data, use get_grib_file.py, inputting the input file and output directory at the top of the file (INPUT_GRIB & OUTPUT_DIR). NOTE: I recommend requesting a small dataset from CDS, because this script can take a while. You can also change the size of the blocks with BLOCK_LAT & BLOCK_LON, and the sparsity of the data by editing TIME_DROP_FRAC, COORD_DROP_FRAC, and VALUE_DROP_FRAC (also at top of file).
2) To do training, use train.py, train_memory_aware.py, train_memory_aware_gpu.py, or train_memory_aware_one_window.py (warning train_memory_aware.py around 10 times longer to run) the only required command line argument --data, which you point to the directory of file you would like to train on
3) Optional arguments are --epochs (defaults to 5), --future-steps (how many future steps we want to predict (defaults to 3), --checkpoint (where the checkpoints on saved on disk, defaults to checkpoint.pth), and --load-checkpoint (which will load a checkpoint if you would like to continue training, put path to checkpoint.pth)
4) Arguments are same for all training algorithms, for the memory aware models you can change the RAM limits at the top of each file. You can also change the number of input timesteps into the ConvLSTM using SEQ_LEN_IN at the top of each relevant file
5) To test, use test.py with the following arguments: --data path to the data, --checkpoint the model you want to test, --future-steps (same as training, defaults to 3), --seq-len (same as SEQ_LEN_IN from training, defaults to 9)
6) Note: Using run_benchmarks.sh will automatically run all 4 algortihms we have 3 times and output the single run results, as well as the averaged results over the 3 runs. You can chang the number of epochs, number of future steps, and the input data at the top of the script.
